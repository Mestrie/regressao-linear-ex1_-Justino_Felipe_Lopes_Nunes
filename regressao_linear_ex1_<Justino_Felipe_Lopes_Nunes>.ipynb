{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcIPOtuBid5Vba36qVtUD2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mestrie/regressao-linear-ex1_-Justino_Felipe_Lopes_Nunes/blob/main/regressao_linear_ex1_%3CJustino_Felipe_Lopes_Nunes%3E.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Functions\n",
        "------------------------------------------------------\n",
        "###compute_cost.py"
      ],
      "metadata": {
        "id": "lSxZyiP-atvM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "prt_nWZLXCjb"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "@file compute_cost.py\n",
        "@brief Computes the cost for linear regression.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def compute_cost(X, y, theta):\n",
        "    \"\"\"\n",
        "    Compute the cost for linear regression.\n",
        "\n",
        "    This function calculates the mean squared error cost function J(θ) for linear regression:\n",
        "    J(θ) = (1 / (2 * m)) * Σ (h(θ) - y)^2\n",
        "\n",
        "    where:\n",
        "    - J(θ) is the cost\n",
        "    - m is the number of training examples\n",
        "    - h(θ) is the hypothesis function (X @ theta)\n",
        "    - y is the vector of observed values\n",
        "\n",
        "    @param X: np.ndarray\n",
        "        Feature matrix including the intercept term (shape: m x n).\n",
        "    @param y: np.ndarray\n",
        "        Target variable vector (shape: m,).\n",
        "    @param theta: np.ndarray\n",
        "        Parameter vector for linear regression (shape: n,).\n",
        "\n",
        "    @return: float\n",
        "        The computed cost value as a single float.\n",
        "    \"\"\"\n",
        "\n",
        "    # get the number of training examples\n",
        "    m = len(y)\n",
        "\n",
        "    # Compute the predictions using the linear model by formula h(θ) = X @ θ\n",
        "    h_o = X @ theta\n",
        "\n",
        "    # Compute the error vector between predictions and actual values\n",
        "    errors = h_o - y\n",
        "\n",
        "    # Compute the cost as the mean squared error cost function using the formula\n",
        "    J_o = (1 / (2 * m)) * np.sum(errors**2)\n",
        "\n",
        "    return J_o\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gradient_descent.py\n",
        "\n"
      ],
      "metadata": {
        "id": "TnVYBDFki04f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "@file gradient_descent.py\n",
        "@brief Implementa o algoritmo de descida do gradiente para regressão linear.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def gradient_descent(X, y, theta, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Executa a descida do gradiente para minimizar a função de custo J(θ)\n",
        "    no contexto de regressão linear.\n",
        "\n",
        "    @return: tuple[np.ndarray, np.ndarray]\n",
        "        theta: vetor otimizado de parâmetros (n,).\n",
        "        J_history: vetor com o histórico do valor da função de custo em cada iteração (num_iters,).\n",
        "        theta_history: parâmetros em cada iteração (num_iters+1, n).\n",
        "    \"\"\"\n",
        "\n",
        "    # Obtem o número de amostras\n",
        "    m = len(y)\n",
        "\n",
        "    # Inicializa o vetor de custo J_history\n",
        "    J_history = np.zeros(num_iters)\n",
        "\n",
        "    # Inicializa o vetor theta_history\n",
        "    theta_history = np.zeros((num_iters + 1, theta.shape[0]))\n",
        "\n",
        "    # Armazena os parâmetros iniciais\n",
        "    theta_history[0] = theta\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        # Calcula as previsões\n",
        "        predictions = X @ theta\n",
        "\n",
        "        # Calcula o erro\n",
        "        erro = predictions - y\n",
        "\n",
        "        # Calcula o gradiente\n",
        "        gradient = (1/m) * (X.T @ erro)\n",
        "\n",
        "        # Atualiza os parâmetros\n",
        "        theta = theta - alpha * gradient\n",
        "\n",
        "        # Armazena o custo da iteração atual\n",
        "        J_history[i] = compute_cost(X, y, theta)\n",
        "\n",
        "        # Armazena os parâmetros atualizados\n",
        "        theta_history[i + 1] = theta\n",
        "\n",
        "    return theta, J_history, theta_history\n"
      ],
      "metadata": {
        "id": "PRAfcMF_jCjR"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}